---
layout: paper
categories: papers
permalink: papers/llmatheval
id: llmatheval
title: "Beyond Final Answers: Evaluating Large Language Models for Math Tutoring"
authors: 
  - Adit Gupta
  - Jennifer Reddig
  - Tommaso Calò
  - Daniel Weitekamp
  - Christopher MacLellan
venue: International Conference on Artificial Intelligence in Education
venue-shorthand: AIED
location: Stockholm, Sweden
year: 2025
url: /papers/llmatheval
pdf: /papers/25-llmatheval-aied.pdf
doi: 10.1007/978-3-031-98414-3_23
type: conference
figure: /images/papers/25-llmatheval-aied.png
selected: false
feature-title: Evaluating LLMs as Math Tutors
feature-description: Comparing correctness and instructional quality of LLMs in algebra tutoring scenarios.
image: /images/papers/25-llmatheval-aied.png
featured: true
feature-order: 3
bibtex: |-

  @inproceedings{gupta2025llmatheval,
  author    = {Adit Gupta and Jennifer Reddig and Tommaso Calò and Daniel Weitekamp and Christopher J. MacLellan},
  title     = {Beyond Final Answers: Evaluating Large Language Models for Math Tutoring},
  booktitle = {Proceedings of the 25th International Conference on Artificial Intelligence in Education (AIED 2025)},
  year      = {2025},
  pages     = {323--337},
  doi       = {10.1007/978-3-031-98414-3_23},
  url       = {https://doi.org/10.1007/978-3-031-98414-3_23}
  }


---
Researchers have made notable progress in applying large language models (LLMs) to solve math problems, as demonstrated through efforts like GSM8k, ProofNet, AlphaGeometry, and MathOdyssey. This progress has sparked interest in their potential use for tutoring students in mathematics. However, the reliability of LLMs in tutoring contexts—where correctness and instructional quality are crucial—remains underexplored. Moreover, LLM problem-solving capabilities may not necessarily translate into effective tutoring support for students. In this work, we present two novel approaches to evaluate the correctness and quality of LLMs in math tutoring contexts. The first approach uses an intelligent tutoring system for college algebra as a testbed to assess LLM problem-solving capabilities. We generate benchmark problems using the tutor, prompt multiple LLMs to solve them, and compare the solutions to those generated by the tutor. The second approach evaluates LLM as tutors rather than problem solvers. We employ human evaluators, who act as students seeking tutoring support from each LLM. We then assess the quality and correctness of the support provided by the LLMs via a qualitative coding process. We applied these methods to evaluate several ChatGPT models, including 3.5 Turbo, 4, 4o, o1-mini, and o1-preview. Our findings show that when used as problem solvers, LLMs generate correct final answers for 85.5% of the college algebra problems tested. When employed interactively as tutors, 90% of LLM dialogues show high-quality instructional support; however, many contain errors—only 56.6% are entirely correct. We conclude that, despite their potential, LLMs are not yet suitable as intelligent tutors for math without human oversight or additional mechanisms to ensure correctness and quality.


